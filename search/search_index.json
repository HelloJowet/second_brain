{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"data_engineering/data_lakehouse/apache_iceberg/","title":"\ud83e\uddca Apache Iceberg","text":""},{"location":"data_engineering/data_lakehouse/apache_iceberg/#rust-package","title":"Rust package","text":""},{"location":"data_engineering/data_lakehouse/apache_iceberg/#dependencies","title":"Dependencies","text":"<pre><code>[dependencies]\niceberg = \"0.4.0\"\ntokio = \"1.42.0\"\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_iceberg/#connect-to-catalog","title":"Connect to Catalog","text":""},{"location":"data_engineering/data_lakehouse/apache_iceberg/#memory-catalog","title":"Memory Catalog","text":"<pre><code>[dependencies]\niceberg-catalog-memory = \"0.4.0\"\n</code></pre> <pre><code>use iceberg::io::FileIOBuilder;\nuse iceberg_catalog_memory::MemoryCatalog;\n\n#[tokio::main()]\nasync fn main() {\n    let file_io = FileIOBuilder::new(\"memory\").build().expect(\"Failed to build file io\");\n    let catalog = MemoryCatalog::new(file_io, None);\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_iceberg/#create-namespace","title":"Create namespace","text":"<pre><code>use std::collections::HashMap;\n\nuse iceberg::NamespaceIdent;\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let namespace_ident = NamespaceIdent::new(\"namespace_01\".to_string());\n    let namespace = catalog\n        .create_namespace(&amp;namespace_ident, HashMap::new())\n        .await\n        .expect(\"Failed to create namespace\");\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_iceberg/#create-table","title":"Create table","text":"<pre><code>use std::collections::HashMap;\n\nuse iceberg::{\n    spec::{NestedField, PrimitiveType, Schema, Type},\n    TableCreation,\n};\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let table_schema = Schema::builder()\n        .with_fields(vec![\n            NestedField::optional(1, \"foo\", Type::Primitive(PrimitiveType::String)).into(),\n            NestedField::required(2, \"bar\", Type::Primitive(PrimitiveType::Int)).into(),\n            NestedField::optional(3, \"baz\", Type::Primitive(PrimitiveType::Boolean)).into(),\n        ])\n        .with_schema_id(1)\n        .with_identifier_field_ids(vec![2])\n        .build()\n        .unwrap();\n\n    let table_creation = TableCreation::builder()\n        .name(\"table_01\".to_string())\n        .location(\"table_01\".to_string())\n        .schema(table_schema.clone())\n        .properties(HashMap::from([(\"owner\".to_string(), \"Jonas Frei\".to_string())]))\n        .build();\n\n    let table = catalog.create_table(&amp;namespace_ident, table_creation).await.unwrap();\n\n    println!(\"Table created: {:?}\", table.metadata());\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_iceberg/#load-table","title":"Load table","text":"<pre><code>use iceberg::TableIdent;\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let table_created = catalog\n        .load_table(&amp;TableIdent::from_strs([\"namespace_01\", \"table_01\"]).unwrap())\n        .await\n        .unwrap();\n    println!(\"{:?}\", table_created.metadata());\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_iceberg/#insert-data","title":"Insert data","text":"<p>The iceberg-rust package currently lacks write support (source).</p>"},{"location":"data_engineering/data_lakehouse/apache_spark/","title":"\u2b50 Apache Spark","text":""},{"location":"data_engineering/data_lakehouse/apache_spark/#deployment","title":"Deployment","text":""},{"location":"data_engineering/data_lakehouse/apache_spark/#docker-compose","title":"Docker compose","text":"<pre><code>services:\n  spark_master:\n    restart: always\n    image: bitnami/spark:3.5\n    ports:\n      - 8080:8080\n      - 7077:7077\n    hostname: spark-master\n    environment:\n      - SPARK_MODE=master\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_WORKER_WEBUI_PORT=8081\n\n  spark_worker:\n    restart: always\n    image: bitnami/spark:3.5\n    ports:\n      - 8081:8081\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_MEMORY=8G\n      - SPARK_WORKER_CORES=4\n      - AWS_ACCESS_KEY_ID=user\n      - AWS_SECRET_ACCESS_KEY=password\n      - AWS_REGION=us-east-1\n    depends_on:\n      - spark_master\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_spark/#python-library","title":"Python library","text":"<p>-&gt; Minio as local s3 service</p>"},{"location":"data_engineering/data_lakehouse/apache_spark/#apache-iceberg-integration","title":"Apache Iceberg integration","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.master('spark://localhost:7077')\n    .config(\n        'spark.jars.packages',\n        'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,' 'org.apache.iceberg:iceberg-aws-bundle:1.7.1,' 'org.postgresql:postgresql:42.7.4',\n    )\n    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n    .config('spark.sql.catalog.my_catalog', 'org.apache.iceberg.spark.SparkCatalog')\n    .config('spark.sql.catalog.my_catalog.type', 'hadoop')\n    .config('spark.sql.catalog.my_catalog.type', 'jdbc')\n    .config('spark.sql.catalog.my_catalog.uri', 'jdbc:postgresql://localhost:5500/postgres')\n    .config('spark.sql.catalog.my_catalog.jdbc.user', 'postgres')\n    .config('spark.sql.catalog.my_catalog.jdbc.password', 'postgres')\n    .config('spark.sql.catalog.my_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n    .config('spark.sql.catalog.my_catalog.warehouse', 's3://data-lakehouse')\n    .config('spark.sql.catalog.my_catalog.s3.region', 'us-east-1')\n    .config('spark.sql.catalog.my_catalog.s3.endpoint', 'http://YOUR_IP_ADDRESS:5561')\n    .config('spark.sql.catalog.my_catalog.s3.access-key-id', 'admin')\n    .config('spark.sql.catalog.my_catalog.s3.secret-access-key', 'password')\n    .getOrCreate()\n)\n\nspark.sql('CREATE TABLE my_catalog.table (name string) USING iceberg;')\nspark.sql(\"INSERT INTO my_catalog.table VALUES ('Alex'), ('Dipankar'), ('Jason')\")\n</code></pre>"},{"location":"data_engineering/data_lakehouse/apache_spark/#apache-iceberg-sedona","title":"Apache Iceberg + Sedona","text":"<pre><code>from sedona.spark import SedonaContext\n\nspark = (\n    SedonaContext.builder()\n    .master('spark://localhost:7077')\n    .config(\n        'spark.jars.packages',\n        # sedona\n        'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0,'\n        'org.datasyslab:geotools-wrapper:1.7.0-28.5,'\n        # iceberg\n        'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,'\n        'org.apache.iceberg:iceberg-aws-bundle:1.7.1,'\n        'org.postgresql:postgresql:42.7.4',\n    )\n    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n    .config('spark.sql.catalog.my_catalog', 'org.apache.iceberg.spark.SparkCatalog')\n    .config('spark.sql.catalog.my_catalog.type', 'jdbc')\n    .config('spark.sql.catalog.my_catalog.uri', 'jdbc:postgresql://localhost:5500/postgres')\n    .config('spark.sql.catalog.my_catalog.jdbc.user', 'postgres')\n    .config('spark.sql.catalog.my_catalog.jdbc.password', 'postgres')\n    .config('spark.sql.catalog.my_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n    .config('spark.sql.catalog.my_catalog.warehouse', 's3://data-lakehouse')\n    .config('spark.sql.catalog.my_catalog.s3.region', 'us-east-1')\n    .config('spark.sql.catalog.my_catalog.s3.endpoint', 'http://YOUR_IP_ADDRESS:5561')\n    .config('spark.sql.catalog.my_catalog.s3.access-key-id', 'admin')\n    .config('spark.sql.catalog.my_catalog.s3.secret-access-key', 'password')\n    .config('spark.sql.catalog.my_catalog.s3.path-style-access', 'true')\n    .getOrCreate()\n)\n\nspark.sql('CREATE TABLE my_catalog.table (name string) USING iceberg;')\nspark.sql(\"INSERT INTO my_catalog.table VALUES ('Alex'), ('Dipankar'), ('Jason')\")\n</code></pre>"},{"location":"data_engineering/data_lakehouse/delta_lake/","title":"\ud83d\udea3\ud83c\udffb Delta Lake","text":""},{"location":"data_engineering/data_lakehouse/delta_lake/#delta-rs-rust","title":"delta-rs (Rust)","text":""},{"location":"data_engineering/data_lakehouse/delta_lake/#dependencies","title":"Dependencies","text":"<pre><code>[dependencies]\ndeltalake = { version = \"0.22.3\", features = [\"datafusion\", \"s3\"] }\ntokio = \"1.42.0\"\n</code></pre> <p>The <code>datafusion</code> feature is needed to write data to a table.</p>"},{"location":"data_engineering/data_lakehouse/delta_lake/#s3-configurations","title":"S3 configurations","text":"<p>Everytime you want to use S3 as a data store, you need to add the following:</p> <pre><code>// Register S3 handlers\ndeltalake::aws::register_handlers(None);\n\n// Set S3 configuration options\nlet mut storage_options = HashMap::new();\nstorage_options.insert(\"AWS_ENDPOINT_URL\".to_string(), \"http://localhost:5561\".to_string());\nstorage_options.insert(\"AWS_REGION\".to_string(), \"us-east-1\".to_string());\nstorage_options.insert(\"AWS_ACCESS_KEY_ID\".to_string(), \"admin\".to_string());\nstorage_options.insert(\"AWS_SECRET_ACCESS_KEY\".to_string(), \"password\".to_string());\nstorage_options.insert(\"AWS_ALLOW_HTTP\".to_string(), \"true\".to_string());\nstorage_options.insert(\"AWS_S3_ALLOW_UNSAFE_RENAME\".to_string(), \"true\".to_string());\n</code></pre> <p>The S3 configuration options could be set as environment variables too.</p> <p>S3 requires a locking provider by default (more information). If you don't want to use a locking provider, you can disable it by setting the <code>AWS_S3_ALLOW_UNSAFE_RENAME</code> variable to <code>true</code>.</p>"},{"location":"data_engineering/data_lakehouse/delta_lake/#create-table","title":"Create table","text":"<pre><code>use deltalake::{\n    kernel::{DataType, StructField},\n    DeltaOps,\n};\n\n#[tokio::main()]\nasync fn main() {\n    let delta_ops = DeltaOps::new_in_memory();\n\n    let columns = vec![\n        StructField::new(\"id\", DataType::INTEGER, false),\n        StructField::new(\"name\", DataType::INTEGER, false),\n        StructField::new(\"age\", DataType::INTEGER, false),\n    ];\n    let table = delta_ops.create().with_table_name(\"employee\").with_columns(columns).await.expect(\"Table creation failed\");\n}\n</code></pre> <p>If you want to save the table in a s3 storage, you need to configure the <code>DeltaOps</code> object differently:</p> <pre><code>use deltalake::{DeltaOps, DeltaTableBuilder};\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let delta_table_builder = DeltaTableBuilder::from_uri(\"s3://data-lakehouse/employee\")\n        .with_storage_options(storage_options)\n        .build()\n        .expect(\"Connection to s3 failed\");\n    let delta_ops = DeltaOps::from(delta_table_builder);\n\n    // ...\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/delta_lake/#insert-data","title":"Insert data","text":"<pre><code>use deltalake::arrow::array::{Int32Array, RecordBatch, StringArray};\nuse deltalake::arrow::datatypes::{DataType as ArrowDataType, Field, Schema as ArrowSchema};\nuse deltalake::DeltaOps;\nuse std::sync::Arc;\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let schema = Arc::new(ArrowSchema::new(vec![\n        Field::new(\"id\", ArrowDataType::Int32, false),\n        Field::new(\"name\", ArrowDataType::Utf8, true),\n    ]));\n\n    // Create employee records\n    let ids = Int32Array::from(vec![1, 2, 3]);\n    let names = StringArray::from(vec![\"Tom\", \"Tim\", \"Titus\"]);\n    let employee_record = RecordBatch::try_new(schema, vec![Arc::new(ids), Arc::new(names)]).unwrap();\n\n    // Insert records\n    let table = DeltaOps(table).write(vec![employee_record]).await.expect(\"Insert failed\");\n}\n</code></pre> <p>The Arrow Rust array primitives are very fickle and so creating a direct transformation is quite tricky in Rust, whereas in Python or another loosely typed language it might be simpler.</p> <p>(source)</p> <p>The default save mode for the <code>delta_ops.write</code> function is <code>SaveMode::Append</code>. To overwrite existing data instead of appending, use <code>SaveMode::Overwrite</code>:</p> <pre><code>use deltalake::protocol::SaveMode;\nuse deltalake::DeltaOps;\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let table = DeltaOps(table)\n        .write(vec![employee_record])\n        .with_save_mode(SaveMode::Overwrite)\n        .await\n        .expect(\"Insert failed\");\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/delta_lake/#load-table","title":"Load table","text":"<p>Open table:</p> <pre><code>#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let table = deltalake::open_table_with_storage_options(\"s3://data-lakehouse/employee\", storage_options)\n        .await\n        .expect(\"Load failed\");\n}\n</code></pre> <p>Load table data:</p> <pre><code>use deltalake::operations::collect_sendable_stream;\nuse deltalake::DeltaOps;\n\n#[tokio::main()]\nasync fn main() {\n    // ...\n\n    let (_, stream) = DeltaOps(table).load().await.expect(\"Load failed\");\n    let records = collect_sendable_stream(stream).await.unwrap();\n\n    println!(\"{:?}\", records)\n}\n</code></pre>"},{"location":"data_engineering/data_lakehouse/delta_lake/#time-travel","title":"Time travel","text":"<p>To load the previous state of a table, you can use the <code>open_table_with_version</code> function:</p> <pre><code>let version = 1;\nlet mut table = deltalake::open_table_with_version(\"s3://data-lakehouse/employee\", version).await.expect(\"Load failed\");\n</code></pre> <p>If the table is already loaded and you want to change the version number, just use the <code>load_version</code> function.</p> <pre><code>table.load_version(2).await.expect(\"Load failed\");\n</code></pre>"},{"location":"data_engineering/data_lakehouse/delta_lake/#examining-table","title":"Examining Table","text":"<p>You can find more information about this inside the official documentation.</p> <pre><code>// Metadata\nprintln!(\"{:?}\", table.metadata().unwrap());\n// Schema\nprintln!(\"{:?}\", table.schema().unwrap());\n// History\nprintln!(\"{:?}\", table.history(None).await.unwrap());\n</code></pre>"},{"location":"data_engineering/data_lakehouse/unity_catalog/","title":"\ud83d\uddc2\ufe0f Unity Catalog","text":""},{"location":"data_engineering/data_lakehouse/unity_catalog/#getting-started-docker","title":"Getting started (Docker)","text":"<ol> <li>Clone repository:    <pre><code>git clone https://github.com/unitycatalog/unitycatalog/tree/main\n</code></pre></li> <li>Start server:    <pre><code>docker build -t unity_catalog_server .\ndocker run -it -p 8080:8080 unity_catalog_server\n</code></pre></li> <li>Start UI:    <pre><code>docker build -t unity_catalog_ui --build-arg PROXY_HOST=YOUR_IP_ADDRESS ui/.\ndocker run -it -p 3000:3000 unity_catalog_ui\n</code></pre></li> </ol>"},{"location":"data_engineering/storage/minio/","title":"\ud83e\udea3 Minio","text":""},{"location":"data_engineering/storage/minio/#docker-compose","title":"Docker compose","text":"<p><code>docker_compose.yaml</code>:</p> <pre><code>services:\n  storage_s3:\n    restart: always\n    image: quay.io/minio/minio:RELEASE.2024-10-29T16-01-48Z\n    ports:\n      - 5560:5560\n      - 5561:5561\n    hostname: storage-s3\n    environment:\n      MINIO_ROOT_USER: admin\n      MINIO_ROOT_PASSWORD: password\n    command: server /data --console-address \":5560\" --address=\":5561\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5560/minio/health/live\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  storage_s3_initial_setup:\n    image: minio/mc:RELEASE.2024-10-29T15-34-59Z\n    depends_on:\n      storage_s3:\n        condition: service_healthy\n    volumes:\n      - ./docker_entrypoint.sh:/docker_entrypoint.sh:z\n    entrypoint:\n      - /docker_entrypoint.sh\n</code></pre> <p><code>docker_entrypoint.sh</code>:</p> <pre><code>#!/bin/bash\n\n# Set up alias for MinIO\nmc alias set minio http://storage-s3:5561 admin password;\n\n# Create buckets\nmc mb minio/data-lakehouse;\n</code></pre>"},{"location":"dev_ops/services/authentication/keycloak/","title":"\ud83d\udddd Keycloak","text":""},{"location":"dev_ops/services/authentication/keycloak/#docker-compose","title":"Docker compose","text":""},{"location":"dev_ops/services/authentication/keycloak/#resolve-local-naming","title":"Resolve local naming","text":"<p>Sometimes Keycloak is accessed via multiple routes. For example <code>host.docker.internal</code> (from a backend) and <code>localhost</code> (from a frontend). To ensure consistent access, an additional hostname is introduced that works for both.</p> <p>Add the following entry to your <code>/etc/hosts</code> file to resolve <code>keycloak.internal</code> to your local machine:</p> <pre><code>echo -e '127.0.0.1\\tkeycloak.internal' | sudo tee -a /etc/hosts\n</code></pre> <p>Verify the setup by running:</p> <pre><code>ping keycloak.internal\n</code></pre> <p>This resolves <code>keycloak.internal</code> to <code>127.0.0.1</code> for both backend and frontend access.</p>"}]}